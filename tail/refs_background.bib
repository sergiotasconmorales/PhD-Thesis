@book{rao2019natural,
  title={Natural language processing with PyTorch: build intelligent language applications using deep learning},
  author={Rao, Delip and McMahan, Brian},
  year={2019},
  publisher={" O'Reilly Media, Inc."}
}

% Text representation
@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@article{sivic2008efficient,
  title={Efficient visual search of videos cast as text retrieval},
  author={Sivic, Josef and Zisserman, Andrew},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={31},
  number={4},
  pages={591--606},
  year={2008},
  publisher={IEEE}
}

@misc{ibmNLGDifferences,
	author = {Eda Kavlakoglu},
	title = {{N}{L}{P} vs. {N}{L}{U} vs. {N}{L}{G}: the differences between three natural language processing concepts - {I}{B}{M} {B}log --- ibm.com},
	howpublished = {\url{https://www.ibm.com/blog/nlp-vs-nlu-vs-nlg-the-differences-between-three-natural-language-processing-concepts/}},
	year = {},
	note = {[Accessed 25-01-2024]},
}
@misc{datasolutNLGUnterschiede,
	author = {Laurenz Wuttke},
	title = {{N}{L}{P} vs. {N}{L}{U} vs. {N}{L}{G}: {U}nterschiede, {F}unktionen und {B}eispiele --- datasolut.com},
	howpublished = {\url{https://datasolut.com/natural-language-processing-vs-nlu-vs-nlg-unterschiede-funktionen-und-beispiele/}},
	year = {},
	note = {[Accessed 25-01-2024]},
}
%resnet paper
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}
@misc{jalammarIllustratedTransformer,
	author = {Jay Alammar},
	title = {{T}he {I}llustrated {T}ransformer --- jalammar.github.io},
	howpublished = {\url{http://jalammar.github.io/illustrated-transformer/}},
	year = {},
	note = {[Accessed 26-01-2024]},
}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}
@misc{ibmWhatRecurrent,
	author = {},
	title = {{W}hat are {R}ecurrent {N}eural {N}etworks? | {I}{B}{M} --- ibm.com},
	howpublished = {\url{https://www.ibm.com/topics/recurrent-neural-networks}},
	year = {},
	note = {[Accessed 26-01-2024]},
}
@article{cho2014learning,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}
@article{schuster1997bidirectional,
  title={Bidirectional recurrent neural networks},
  author={Schuster, Mike and Paliwal, Kuldip K},
  journal={IEEE transactions on Signal Processing},
  volume={45},
  number={11},
  pages={2673--2681},
  year={1997},
  publisher={Ieee}
}
@misc{nvidiaWhatLarge,
	author = {},
	title = {{W}hat are {L}arge {L}anguage {M}odels? | {N}{V}{I}{D}{I}{A} {G}lossary --- nvidia.com},
	howpublished = {\url{https://www.nvidia.com/en-us/glossary/large-language-models/}},
	year = {},
	note = {[Accessed 27-01-2024]},
}
@article{smith2022using,
  title={Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model},
  author={Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and others},
  journal={arXiv preprint arXiv:2201.11990},
  year={2022}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@misc{nvidiaUsingDeepSpeed,
	author = {},
	title = {{U}sing {D}eep{S}peed and {M}egatron to {T}rain {M}egatron-{T}uring {N}{L}{G} 530{B}, the {W}orld’s {L}argest and {M}ost {P}owerful {G}enerative {L}anguage {M}odel | {N}{V}{I}{D}{I}{A} {T}echnical {B}log --- developer.nvidia.com},
	howpublished = {\url{https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/}},
	year = {},
	note = {[Accessed 27-01-2024]},
}
@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}
@article{ma2023eureka,
    title   = {Eureka: Human-Level Reward Design via Coding Large Language Models},
    author  = {Yecheng Jason Ma and William Liang and Guanzhi Wang and De-An Huang and Osbert Bastani and Dinesh Jayaraman and Yuke Zhu and Linxi Fan and Anima Anandkumar},
    year    = {2023},
    journal = {arXiv preprint arXiv: Arxiv-2310.12931}
}
@article{van2023clinical,
  title={Clinical text summarization: Adapting large language models can outperform human experts},
  author={Van Veen, Dave and Van Uden, Cara and Blankemeier, Louis and Delbrouck, Jean-Benoit and Aali, Asad and Bluethgen, Christian and Pareek, Anuj and Polacin, Malgorzata and Collins, William and Ahuja, Neera and others},
  journal={arXiv preprint arXiv:2309.07430},
  year={2023}
}
@article{schubert2023large,
  title={Large Language Model-Driven Evaluation of Medical Records Using MedCheckLLM},
  author={Schubert, Marc Cicero and Wick, Wolfgang and Venkataramani, Varun},
  journal={medRxiv},
  pages={2023--11},
  year={2023},
  publisher={Cold Spring Harbor Laboratory Press}
}
@article{pal2023chatgpt,
  title={ChatGPT or LLM in next-generation drug discovery and development: pharmaceutical and biotechnology companies can make use of the artificial intelligence-based device for a faster way of drug discovery and development},
  author={Pal, Soumen and Bhattacharya, Manojit and Islam, Md Aminul and Chakraborty, Chiranjib},
  journal={International Journal of Surgery},
  volume={109},
  number={12},
  pages={4382--4384},
  year={2023},
  publisher={LWW}
}
@article{workshop2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}
@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
@misc{li2021prefixtuning,
      title={Prefix-Tuning: Optimizing Continuous Prompts for Generation}, 
      author={Xiang Lisa Li and Percy Liang},
      year={2021},
      eprint={2101.00190},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{houlsby2019parameterefficient,
      title={Parameter-Efficient Transfer Learning for NLP}, 
      author={Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
      year={2019},
      eprint={1902.00751},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{xu2023parameterefficient,
      title={Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment}, 
      author={Lingling Xu and Haoran Xie and Si-Zhao Joe Qin and Xiaohui Tao and Fu Lee Wang},
      year={2023},
      eprint={2312.12148},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{dubey2022activation,
  title={Activation functions in deep learning: A comprehensive survey and benchmark},
  author={Dubey, Shiv Ram and Singh, Satish Kumar and Chaudhuri, Bidyut Baran},
  journal={Neurocomputing},
  year={2022},
  publisher={Elsevier}
}
@article{gholamalinezhad2020pooling,
  title={Pooling methods in deep neural networks, a review},
  author={Gholamalinezhad, Hossein and Khosravi, Hossein},
  journal={arXiv preprint arXiv:2009.07485},
  year={2020}
}
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}
@misc{radford2021learning,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@inproceedings{kazemzadeh2014referitgame,
  title={Referitgame: Referring to objects in photographs of natural scenes},
  author={Kazemzadeh, Sahar and Ordonez, Vicente and Matten, Mark and Berg, Tamara},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={787--798},
  year={2014}
}
@misc{karpathy2015deep,
      title={Deep Visual-Semantic Alignments for Generating Image Descriptions}, 
      author={Andrej Karpathy and Li Fei-Fei},
      year={2015},
      eprint={1412.2306},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@inproceedings{lee2018stacked,
  title={Stacked cross attention for image-text matching},
  author={Lee, Kuang-Huei and Chen, Xi and Hua, Gang and Hu, Houdong and He, Xiaodong},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={201--216},
  year={2018}
}
@misc{hebert2024multimodal,
      title={Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media}, 
      author={Liam Hebert and Gaurav Sahu and Yuxuan Guo and Nanda Kishore Sreenivas and Lukasz Golab and Robin Cohen},
      year={2024},
      eprint={2307.09312},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{yang2016stacked,
      title={Stacked Attention Networks for Image Question Answering}, 
      author={Zichao Yang and Xiaodong He and Jianfeng Gao and Li Deng and Alex Smola},
      year={2016},
      eprint={1511.02274},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{ren2015faster,
  title={Faster r-cnn: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@inproceedings{xu2016ask,
  title={Ask, attend and answer: Exploring question-guided spatial attention for visual question answering},
  author={Xu, Huijuan and Saenko, Kate},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part VII 14},
  pages={451--466},
  year={2016},
  organization={Springer}
}
@inproceedings{jiang2020defense,
  title={In defense of grid features for visual question answering},
  author={Jiang, Huaizu and Misra, Ishan and Rohrbach, Marcus and Learned-Miller, Erik and Chen, Xinlei},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10267--10276},
  year={2020}
}
@inproceedings{liang2020learning,
  title={Learning to contrast the counterfactual samples for robust visual question answering},
  author={Liang, Zujie and Jiang, Weitao and Hu, Haifeng and Zhu, Jiaying},
  booktitle={Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)},
  pages={3285--3292},
  year={2020}
}
@misc{khan2021reason,
      title={Found a Reason for me? Weakly-supervised Grounded Visual Question Answering using Capsules}, 
      author={Aisha Urooj Khan and Hilde Kuehne and Kevin Duarte and Chuang Gan and Niels Lobo and Mubarak Shah},
      year={2021},
      eprint={2105.04836},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{wang2022ofa,
      title={OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework}, 
      author={Peng Wang and An Yang and Rui Men and Junyang Lin and Shuai Bai and Zhikang Li and Jianxin Ma and Chang Zhou and Jingren Zhou and Hongxia Yang},
      year={2022},
      eprint={2202.03052},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{guo2023images,
      title={From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models}, 
      author={Jiaxian Guo and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Boyang Li and Dacheng Tao and Steven C. H. Hoi},
      year={2023},
      eprint={2212.10846},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{li2023blip2,
      title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models}, 
      author={Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
      year={2023},
      eprint={2301.12597},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{gao2023llamaadapter,
      title={LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model}, 
      author={Peng Gao and Jiaming Han and Renrui Zhang and Ziyi Lin and Shijie Geng and Aojun Zhou and Wei Zhang and Pan Lu and Conghui He and Xiangyu Yue and Hongsheng Li and Yu Qiao},
      year={2023},
      eprint={2304.15010},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{zhang2023llamaadapter,
      title={LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention}, 
      author={Renrui Zhang and Jiaming Han and Chris Liu and Peng Gao and Aojun Zhou and Xiangfei Hu and Shilin Yan and Pan Lu and Hongsheng Li and Yu Qiao},
      year={2023},
      eprint={2303.16199},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{wang2024qa,
      title={Q\&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge}, 
      author={Haibi Wang and Weifeng Ge},
      year={2024},
      eprint={2401.10712},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{kim2024generalizing,
      title={Generalizing Visual Question Answering from Synthetic to Human-Written Questions via a Chain of QA with a Large Language Model}, 
      author={Taehee Kim and Yeongjae Cho and Heejun Shin and Yohan Jo and Dongmyung Shin},
      year={2024},
      eprint={2401.06400},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{naseem2022vision,
  title={Vision-language transformer for interpretable pathology visual question answering},
  author={Naseem, Usman and Khushi, Matloob and Kim, Jinman},
  journal={IEEE Journal of Biomedical and Health Informatics},
  volume={27},
  number={4},
  pages={1681--1690},
  year={2022},
  publisher={IEEE}
}
@misc{vansonsbeek2023openended,
      title={Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models}, 
      author={Tom van Sonsbeek and Mohammad Mahdi Derakhshani and Ivona Najdenkoska and Cees G. M. Snoek and Marcel Worring},
      year={2023},
      eprint={2303.05977},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{seenivasan2023surgicalgpt,
  title={SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery},
  author={Seenivasan, Lalithkumar and Islam, Mobarakol and Kannan, Gokul and Ren, Hongliang},
  journal={arXiv preprint arXiv:2304.09974},
  year={2023}
}
@misc{he2024pefomed,
      title={PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering}, 
      author={Jinlong He and Pengfei Li and Gang Liu and Zixu Zhao and Shenjun Zhong},
      year={2024},
      eprint={2401.02797},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{singh2019towards,
  title={Towards vqa models that can read},
  author={Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8317--8326},
  year={2019}
}
@misc{marino2019okvqa,
      title={OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge}, 
      author={Kenneth Marino and Mohammad Rastegari and Ali Farhadi and Roozbeh Mottaghi},
      year={2019},
      eprint={1906.00067},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@inproceedings{kovaleva2020towards,
  title={Towards visual dialog for radiology},
  author={Kovaleva, Olga and Shivade, Chaitanya and Kashyap, Satyananda and Kanjaria, Karina and Wu, Joy and Ballah, Deddeh and Coy, Adam and Karargyris, Alexandros and Guo, Yufan and Beymer, David Beymer and others},
  booktitle={Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing},
  pages={60--69},
  year={2020}
}
@article{zhang2023pmc,
  title={Pmc-vqa: Visual instruction tuning for medical visual question answering},
  author={Zhang, Xiaoman and Wu, Chaoyi and Zhao, Ziheng and Lin, Weixiong and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},
  journal={arXiv preprint arXiv:2305.10415},
  year={2023}
}
@Inproceedings{ImageCLEFVQA-Med2019,
author = {Asma {Ben Abacha} and Sadid A. Hasan and Vivek V. Datla and Joey Liu and Dina Demner-Fushman and Henning M\"uller},
title = {{VQA-Med}: Overview of the Medical Visual Question Answering Task at ImageCLEF 2019},
booktitle = {CLEF2019 Working Notes},
series = {{CEUR} Workshop Proceedings},
year = {2019},
volume = {},
publisher = {CEUR-WS.org $<$http://ceur-ws.org$>$},
pages = {},
month = {9},
address = {Lugano, Switzerland},
}

@Inproceedings{ImageCLEF-VQA-Med2020,
author = {Asma {Ben Abacha} and Vivek V. Datla and Sadid A. Hasan and Dina Demner-Fushman and Henning M\"uller},
title = {Overview of the VQA-Med Task at ImageCLEF 2020: Visual Question Answering and Generation in the Medical Domain}, 
booktitle = {CLEF 2020 Working Notes},
series = {{CEUR} Workshop Proceedings},
year = {2020},
volume = {},
publisher = {CEUR-WS.org},
pages = {},
month = {9},
address = {Thessaloniki, Greece}
}
@Inproceedings{ImageCLEF-VQA-Med2021,
author = {Asma {Ben Abacha} and Mourad Sarrouti and Dina Demner-Fushman and Sadid A. Hasan and Henning M\"uller},
title = {Overview of the VQA-Med Task at ImageCLEF 2021: Visual Question Answering and Generation in the Medical Domain},
booktitle = {CLEF 2021 Working Notes},
series = {{CEUR} Workshop Proceedings},
year = {2021},
volume = {},
publisher = {CEUR-WS.org},
pages = {},
month = {9},
address = {Bucharest, Romania}
}
@inproceedings{ImageCLEF2023,
author = {Bogdan Ionescu and Henning Müller and Ana-Maria Druagulinescu and Wen-wai Yim and Asma {Ben Abacha} and Neal Snider and others},
title = {{Overview of ImageCLEF 2023}: Multimedia Retrieval in Medical, SocialMedia and Recommender Systems Applications},
booktitle = {Experimental IR Meets Multilinguality, Multimodality, and Interaction},
series = {Proceedings of the 14th International Conference of the CLEF Association (CLEF 2023)},
year = {2023},
publisher = {Springer Lecture Notes in Computer Science LNCS},
pages = {},
month = {9},
address = {Thessaloniki, Greece}
}
@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}
@misc{yin2023survey,
      title={A Survey on Multimodal Large Language Models}, 
      author={Shukang Yin and Chaoyou Fu and Sirui Zhao and Ke Li and Xing Sun and Tong Xu and Enhong Chen},
      year={2023},
      eprint={2306.13549},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{tong2024eyes,
  title={Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs},
  author={Tong, Shengbang and Liu, Zhuang and Zhai, Yuexiang and Ma, Yi and LeCun, Yann and Xie, Saining},
  journal={arXiv preprint arXiv:2401.06209},
  year={2024}
}
@misc{dai2023instructblip,
      title={InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning}, 
      author={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
      year={2023},
      eprint={2305.06500},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{liu2023visual,
      title={Visual Instruction Tuning}, 
      author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
      year={2023},
      eprint={2304.08485},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{openai2023gpt4v,
	author = {},
	title = {GPT-4V(ision) System Card},
	howpublished = {\url{https://cdn.openai.com/papers/GPTV_System_Card.pdf}},
	year = {},
	note = {[Accessed 31-01-2024]},
}
@misc{geminiteam2023gemini,
      title={Gemini: A Family of Highly Capable Multimodal Models}, 
      author={Gemini Team and Rohan Anil and Sebastian Borgeaud and Yonghui Wu and Jean-Baptiste Alayrac and Jiahui Yu and others},
      year={2023},
      eprint={2312.11805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{wang2023r2gengpt,
  title={R2gengpt: Radiology report generation with frozen llms},
  author={Wang, Zhanyu and Liu, Lingqiao and Wang, Lei and Zhou, Luping},
  journal={Meta-Radiology},
  volume={1},
  number={3},
  pages={100033},
  year={2023},
  publisher={Elsevier}
}
@article{mesko2023impact,
  title={The impact of multimodal large language models on health care’s future},
  author={Mesk{\'o}, Bertalan},
  journal={Journal of Medical Internet Research},
  volume={25},
  pages={e52865},
  year={2023},
  publisher={JMIR Publications Toronto, Canada}
}
@inproceedings{wu2023multimodal,
  title={Multimodal large language models: A survey},
  author={Wu, Jiayang and Gan, Wensheng and Chen, Zefeng and Wan, Shicheng and Philip, S Yu},
  booktitle={2023 IEEE International Conference on Big Data (BigData)},
  pages={2247--2256},
  year={2023},
  organization={IEEE}
}
@InProceedings{Li_2023_WACV,
    author    = {Li, Haopeng and Ke, Qiuhong and Gong, Mingming and Drummond, Tom},
    title     = {Progressive Video Summarization via Multimodal Self-Supervised Learning},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {1},
    year      = {2023},
    pages     = {5584-5593}
}

@inproceedings{cui2024survey,
  title={A survey on multimodal large language models for autonomous driving},
  author={Cui, Can and Ma, Yunsheng and Cao, Xu and Ye, Wenqian and Zhou, Yang and Liang, Kaizhao and Chen, Jintai and Lu, Juanwu and Yang, Zichong and Liao, Kuei-Da and others},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={958--979},
  year={2024}
}
@misc{wang2023visionllm,
      title={VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks}, 
      author={Wenhai Wang and Zhe Chen and Xiaokang Chen and Jiannan Wu and Xizhou Zhu and Gang Zeng and Ping Luo and Tong Lu and Jie Zhou and Yu Qiao and Jifeng Dai},
      year={2023},
      eprint={2305.11175},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{li2023otter,
  title={Otter: A multi-modal model with in-context instruction tuning},
  author={Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Yang, Jingkang and Liu, Ziwei},
  journal={arXiv preprint arXiv:2305.03726},
  year={2023}
}
@misc{li2024videochat,
      title={VideoChat: Chat-Centric Video Understanding}, 
      author={KunChang Li and Yinan He and Yi Wang and Yizhuo Li and Wenhai Wang and Ping Luo and Yali Wang and Limin Wang and Yu Qiao},
      year={2024},
      eprint={2305.06355},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{zhang2024mmllms,
      title={MM-LLMs: Recent Advances in MultiModal Large Language Models}, 
      author={Duzhen Zhang and Yahan Yu and Chenxing Li and Jiahua Dong and Dan Su and Chenhui Chu and Dong Yu},
      year={2024},
      eprint={2401.13601},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}
@article{dong2022survey,
  title={A survey for in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}

@article{varas2023innovations,
  title={Innovations in surgical training: exploring the role of artificial intelligence and large language models (LLM)},
  author={Varas, Julian and Coronel, Brandon Valencia and VILLAGR{\'a}N, IGNACIO and Escalona, Gabriel and Hernandez, Rocio and Schuit, Gregory and DUR{\'a}N, VALENTINA and Lagos-Villaseca, Antonia and Jarry, Cristian and Neyem, Andres and others},
  journal={Revista do Col{\'e}gio Brasileiro de Cirurgi{\~o}es},
  volume={50},
  pages={e20233605},
  year={2023},
  publisher={SciELO Brasil}
}
@article{mohapatra2023leveraging,
  title={Leveraging Large Language Models (LLM) for the Plastic Surgery Resident Training: Do They Have a Role?},
  author={Mohapatra, Devi Prasad and Thiruvoth, Friji Meethale and Tripathy, Satyaswarup and Rajan, Sheeja and Vathulya, Madhubari and Lakshmi, Palukuri and Singh, Veena K and Haq, Ansar Ul},
  journal={Indian Journal of Plastic Surgery},
  volume={56},
  number={05},
  pages={413--420},
  year={2023},
  publisher={Thieme Medical and Scientific Publishers Pvt. Ltd. A-12, 2nd Floor, Sector 2~…}
}
@misc{forbesCouncilPost,
	author = {Kunal Agarwal},
	title = {{C}ouncil {P}ost: {W}hy {O}ptimizing {C}ost {I}s {C}rucial {T}o {A}{I}/{M}{L} {S}uccess --- forbes.com},
	howpublished = {\url{https://www.forbes.com/sites/forbestechcouncil/2023/09/13/why-optimizing-cost-is-crucial-to-aiml-success/}},
	year = {},
	note = {[Accessed 02-02-2024]},
}
@misc{touvron2021training,
      title={Training data-efficient image transformers \& distillation through attention}, 
      author={Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Hervé Jégou},
      year={2021},
      eprint={2012.12877},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{liu2021swin,
      title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows}, 
      author={Ze Liu and Yutong Lin and Yue Cao and Han Hu and Yixuan Wei and Zheng Zhang and Stephen Lin and Baining Guo},
      year={2021},
      eprint={2103.14030},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{alayrac2022flamingo,
      title={Flamingo: a Visual Language Model for Few-Shot Learning}, 
      author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
      year={2022},
      eprint={2204.14198},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@inproceedings{al2020inception,
  title={The Inception Team at VQA-Med 2020: Pretrained VGG with Data Augmentation for Medical VQA and VQG.},
  author={Al-Sadi, Aisha and Hana'Al-Theiabat, Al-Ayyoub M and Al-Ayyoub, Mahmoud},
  booktitle={CLEF (Working Notes)},
  year={2020}
}
@inproceedings{mathew2021docvqa,
  title={Docvqa: A dataset for vqa on document images},
  author={Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV},
  booktitle={Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages={2200--2209},
  year={2021}
}
@article{geman2015visual,
  title={Visual turing test for computer vision systems},
  author={Geman, Donald and Geman, Stuart and Hallonquist, Neil and Younes, Laurent},
  journal={Proceedings of the National Academy of Sciences},
  volume={112},
  number={12},
  pages={3618--3623},
  year={2015},
  publisher={National Acad Sciences}
}
@inproceedings{bigham2010vizwiz,
  title={Vizwiz: nearly real-time answers to visual questions},
  author={Bigham, Jeffrey P and Jayant, Chandrika and Ji, Hanjie and Little, Greg and Miller, Andrew and Miller, Robert C and Miller, Robin and Tatarowicz, Aubrey and White, Brandyn and White, Samual and others},
  booktitle={Proceedings of the 23nd annual ACM symposium on User interface software and technology},
  pages={333--342},
  year={2010}
}
@article{malinowski2014multi,
  title={A multi-world approach to question answering about real-world scenes based on uncertain input},
  author={Malinowski, Mateusz and Fritz, Mario},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}
@article{tu2014joint,
  title={Joint video and text parsing for understanding events and answering queries},
  author={Tu, Kewei and Meng, Meng and Lee, Mun Wai and Choe, Tae Eun and Zhu, Song-Chun},
  journal={IEEE MultiMedia},
  volume={21},
  number={2},
  pages={42--70},
  year={2014},
  publisher={IEEE}
}
@misc{sightresearchukYourEyes,
	author = {Sight Research UK},
	title = {{H}ow {Y}our {E}yes {W}ork --- sightresearchuk.org},
	howpublished = {\url{https://www.sightresearchuk.org/how-your-eyes-work/}},
	year = {},
	note = {[Accessed 16-02-2024]},
}
@article{bernardes2011digital,
  title={Digital ocular fundus imaging: a review},
  author={Bernardes, Rui and Serranho, Pedro and Lobo, Concei{\c{c}}{\~a}o},
  journal={Ophthalmologica},
  volume={226},
  number={4},
  pages={161--181},
  year={2011},
  publisher={S. Karger AG Basel, Switzerland}
}

@article{zhu2012eye,
  title={Eye anatomy},
  author={Zhu, Jie and Zhang, Ellean and Del Rio-Tsonis, Katia},
  journal={eLS},
  year={2012},
  publisher={Wiley Online Library}
}
@article{gao2024lora,
  title={Lora: A logical reasoning augmented dataset for visual question answering},
  author={Gao, Jingying and Wu, Qi and Blair, Alan and Pagnucco, Maurice},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{yang2023event,
  title={Event-Oriented Visual Question Answering: The E-VQA Dataset and Benchmark},
  author={Yang, Zhenguo and Xiang, Jiale and You, Jiuxiang and Li, Qing and Liu, Wenyin},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2023},
  publisher={IEEE}
}
