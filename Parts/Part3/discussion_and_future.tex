\chapter{Discussion and Conclusion}
\label{chapter:discussion_conclusion}
% Information about how LLMs help with the consistency problem

The previous four chapters presented solutions addressing two specific challenges in \gls{medvqa}: localized questions and consistency. On the one hand, localized questions (\ie, inquiries about specific regions of an image) enhance the ability to perform localized assessments of image contents, proving particularly valuable in diagnosing and offering second opinions on suspicious regions. Furthermore, having the possibility of asking localized questions has implications for model evaluation, allowing for the examination of agreement both within localized questions and between local and global questions. This approach provides more nuanced insights into the model's actual visual understanding. On the other hand, consistency directly involves the quality of the model's reasoning. Avoiding contradictions and correctly quantifying them becomes a crucial aspect of \gls{medvqa} models, influencing their trustworthiness and potential applicability in the medical practice. 

In this chapter, we provide a summary of the findings of the presented works, and discuss their relevance, limitations and significance.

\newpage


\section{Discussion}

\subsection{Localized Questions}

In Chapters~\ref{chapter:locvqa} and~\ref{chapter:locvqallm}, we introduced two methods that enable asking localized questions in \gls{vqa}, with a focus on medical images. The first approach (Chapter~\ref{chapter:locvqa}) utilizes the traditional guided-attention mechanism, allowing the model to learn, for a given question, which parts of the image are most relevant. We employ a binary mask to integrate the region's location information in such a way that the model initially considers the evidence from the whole image, and then restricts the attention to the region. In the second approach (Chapter~\ref{chapter:locvqallm}), we extend localized questions to \glspl{mllm} by creating targeted visual prompts, involving a customized visual prompt with information about the region and its context in the image. 

In both of the presented works, the significance of context became apparent in the results, to the extent that a model can rely solely on context to achieve high performance, specially when the regions encompass only a small portion of a much larger object (Sec.~\ref{sec:locvqallm_results}). While this observation is evident, it raises the question of how much emphasis should be placed on context when answering a localized question. That is, to what extent should the global understanding of the image influence the final answer to the localized question. We argue that this depends on the size of the region relative to the object(s) about which the question is posed. In cases where the region fully contains the object, more importance could be assigned to the region rather than the context. In other scenarios, our localized attention method (Chapter~\ref{chapter:locvqa}) suggests that the context should be leveraged in a sequential manner, mirroring the way a human would answer the question: first considering the entire image in relation to the question to identify relevant structures, and then focusing attention on the region to make a more detailed decision about its contents. Our method in Chapter~\ref{chapter:locvqa} replicates this by injecting the binary mask of the target region into the attention mechanism. One limitation of this way of incorporating the region is that the sub-sampling process of the mask can remove details from the contour of the region, potentially causing errors specially when the region is not rectangular. Our second methods mitigates this issue by allowing the model to separately analyze both the entire image and the region. 

In Chapter~\ref{chapter:locvqallm}, the method presented demonstrates its effectiveness in integrating region-based queries into \glspl{mllm}. In essence, the findings align closely with those of our localized attention method. Specifically, the targeted visual prompting method proves to be more effective in datasets like DME-VQA or RIS-VQA, where both the region's contents and the context are mutually crucial for answering the questions. These datasets also exhibit fewer spurious correlations between the region's location/size and its answer, a characteristic that, in the case of INSEGCAT-VQA, leads to certain baselines relying on shortcuts.

% Also consider limitations of the proposed methods e.g., dataset standard questions
Another crucial aspect to consider regarding localized questions is their applicability to real world scenarios in general \gls{vqa} vs. medical \gls{vqa}. In this regard, we sustain that applications in the medical domain are easier to envision, due to their potential usefulness in diagnosis, where a localized understanding of the image tends to be more relevant. In the presented works, we make use of three medical datasets. The surgical datasets (RIS-VQA and INSEGCAT-VQA) were created due to the availability of the segmentation annotations, but they do not entirely illustrate a real-world application of localized questions, since the usefulness of the questions is restricted to the detection of surgical instruments. 

For natural images, region-defined questions tend to be asked less, in part because of the human tendency to point instead of selecting a region~\cite{mani2020point}. This, however, does not imply that this type of questions is irrelevant for natural images. As discussed earlier, the evaluation of a model's compositional understanding of and reasoning about the reality expressed by an image is an important emerging field. One possible application of localized questions for natural images is the evaluation of a model's reasoning, as discussed later in Sec.~\ref{subsec:the_full_picture}.

\subsection{Consistency Enhancement}
% Also consider limitations of the proposed methods e.g., requires implication annotations
Chapters~\ref{chapter:cons_mainsub} and~\ref{chapter:cons_logic} introduced two methodologies for enhancing consistency in \gls{vqa} models. Both approaches leverage a specialized loss function term during training to penalize instances of inconsistency. The primary distinction between the two lies in the definition of consistency and the mathematical function employed to formulate the specialized loss term. In the first method (Chapter~\ref{chapter:cons_mainsub}), questions are categorized as either main (reasoning) or sub (perception), based on the abstract reasoning level required from the model to answer them. This categorization is used to penalize inconsistent cases with a loss term employing cross-entropy as a measure of correctness. In the second method (Chapter~\ref{chapter:cons_logic}), a more general framework is proposed. Here, QA pairs are treated as propositions, enabling the application of a more general definition of consistency to pairs linked by an implication relation. Inconsistent cases are then penalized using a loss function that provides high values whenever inconsistent cases occur, contributing to the overall robustness of the model.


% Highlight importance of inconsistencies in reasoning
% Link previous item with imporitant of a formal general definition of inconsistency
Given the pivotal role of consistency in reasoning~\cite{selvaraju2020squinting,yuan2021perception,jing2022maintaining}, models exhibiting fewer contradictory or inconsistent answers are perceived as better reasoners. A model that answers ``yes" to both ``is the pizza vegetarian?" and ``is there chicken on the pizza?" is indicative of reasoning issues. As observed, the origin of such contradictions may reside in one or more \gls{vqa} elements. The language model might struggle with accurate associations, the vision encoder could generate erroneous features, or the block that combines language and vision might encounter difficulties in aligning features. Additionally, the issue might stem from the data itself, where the dataset's distribution affects the model's ability to develop a comprehensive understanding of terms like "vegetarian" due to insufficient examples or biases present in the dataset. In our presented approaches, we adopt a more comprehensive approach, considering the entire \gls{vqa} model, and strive to enhance consistency for the given dataset.

% Distinction between reasoning and perception is not trivial
To enhance consistency, it is crucial to establish an appropriate definition that is both general enough to encompass any pair of QA pairs and robust enough to avoid under and overcounting inconsistent cases. In Chapter~\ref{chapter:cons_mainsub}, following~\cite{selvaraju2020squinting}, we use a definition of consistency based on the distinction between reasoning and perception questions. Under this definition, two QA pairs are inconsistent for a given image if the answer to the main question is correct while the answer to the sub-question is incorrect. One limitation of this definition is its subjectivity in distinguishing between reasoning and perception; for instance, the question ``is the car damaged?" could be perceived as either reasoning or perception, depending on the image depicting the car. A picture of a car with scratches and dents would require simply the detection of such deformations (perception), whereas a picture of a car in an accident scene with oil leaks would require the composition of various perception tasks and prior knowledge.

% Assuming same relation between QA pairs is bad (give examples from Introspect). Find a case in which main question does not imply the sub-question. E.g. (1) Is the person about to do a trick? No (2) Is the person sitting? Yes. In this case (2) is sub but it implies the main.
Another flaw in this definition is its exclusive focus on questions, neglecting the consideration of answers. Furthermore, it assumes an implication relation from the main question $q_m$ to the sub-question $q_s$ (i.e., $q_m \rightarrow q_s$). This assumption is not always valid and stems from the disregard of the answers in the categorization process. For instance, consider the pairs ($q_m: \,$``is the person about to do a trick?", $a_m:$ ``no") and ($q_s: \,$``is the person sitting?", $a_s:$ ``yes"). In this scenario, the actual implication relation, considering both questions and answers, is $(q_m, a_m) \leftarrow (q_s, a_s)$. If the model predicts ``no" to both questions (\ie, the sub-question is incorrect but the main question is correct), the pair, according to this definition, would be counted as inconsistent. This, however, overlooks the fact that there is no inherent contradiction between a person not being about to do a trick and a person not sitting.

% our definition is better and more general
% Still, mention that it is hard sometimes to establish implication due to relativity of the propositions (e.g. snow in winter)
% Link previous to importance of dataset quality
To address these issues, Chapter~\ref{chapter:cons_logic} introduces a general definition of consistency for \gls{vqa} that relies on the specific modal relation that exists between QA pairs, which are treated as propositions. According to this definition, a pair of QA is considered inconsistent if the propositions implied by them cannot simultaneously be true. This definition is useful for imposing consistency at training time as well as for measuring consistency, as demonstrated in our study. However, a drawback of this approach lies in the assignment of implications to pairs of propositions, as the validity of such implications is not universally applicable in many cases. For example, in the pair, $p= \,$(``is it summer?", ``no") and $q= \,$(``is there snow?", "yes"), the implication $p \leftarrow q$ may generally hold, but exceptions exist (\eg, a glacier in Switzerland or a city in Norway) where the presence of snow does not necessarily negate the occurrence of summer. In this case, we argue that adopting the most general implication ($p \leftarrow q$) is more advantageous for the model. The model would naturally learn exceptions to these general rules based on the samples within the dataset. Consequently, the accuracy and quality of data annotations become critical for developing models with enhanced consistency.

% Consistency vs. accuracy
% Limitation: Convergence of loss term and cross-entropy do not happen at the same time
When examining consistency, a critical aspect that deserves analysis is its relationship with accuracy. Since a higher consistency does not necessarily imply increased accuracy, it is worth breaking down the potential interactions that can occur for a pair of QA (or propositions). Considering the related binary QA pairs $(q_1, a_1) \rightarrow (q_2, a_2)$, as described in Chapter~\ref{chapter:cons_logic}, the pair will be deemed inconsistent when the model produces answers $\hat{a}_1$ and $\hat{a}_2$ satisfying $\hat{a}_1 = a_1$ and $\hat{a}_2 = \neg a_2$. In the ideal scenario, the model would correct the answer to $q_2$ from $\neg a_2$ to $a_2$ while maintaining $\hat{a}_1 = a_1$, thereby resolving the inconsistency and improving accuracy simultaneously. However, two other outcomes could also render the pair consistent: $(\neg a_1, a_2)$ and $(\neg a_1, \neg a_2)$. The former enhances consistency while maintaining the same accuracy, and the latter improves consistency at the expense of accuracy. In contrast to previously proposed methods that compromise accuracy~\cite{selvaraju2020squinting,ribeiro2019red,goel2021iq}, our proposed methods demonstrate that the model is encouraged to rectify inconsistencies by altering the incorrect answer in the pair, as opposed to changing both answers or modifying the one corresponding to the sufficient condition. This is a relevant outcome under the consideration that regularizers can reach convergence after the main loss term~\cite{teney2019incorporating}. 


% Limitation for M1: Threshold value must be selected
% Limitation in logic: Requires annotations (but they are text based)
Regarding the mathematical definitions of the proposed terms, certain limitations can be identified. In the method presented in Chapter~\ref{chapter:cons_mainsub}, the mathematical function requires a hyperparameter $\gamma$ to determine at which value of the cross-entropy of the main question the penalty should be disabled. While effective in enhancing consistency, this loss term requires a certain level of heuristic exploration to find the most effective value for $\gamma$, making the process somewhat tedious. The generality of our second method removes the need to find the optimal value of a hyperparameter other than the loss term gain. This, coupled with the absence of abrupt changes in the function, constitutes a significant improvement. 

%Finally, a rigorous study on consistency is desirable, employing large \glspl{vlm} across multiple datasets. This exploration would facilitate assessing the degree to which heightened capacity enhances consistency and quantifying any persisting inconsistent cases requiring correction. In pursuit of this objective, various open-source \gls{mllm} can be applied to both medical and non-medical datasets.


% Advantage of logic: Annotations do not need to agree with GT because model can learn that if P->Q then ~Q->~P


\subsection{Bridging Locality and Consistency}
\label{subsec:the_full_picture}

% Contradictions global vs. global move to global vs. local and local vs. local -> this is what constitutes beyond the full picture.

Localized questions and consistency, as discussed, are individually significant for localized examination of images and reasoning, respectively. We will now explore how incorporating localized questions into consistency enhancement can broaden the possibilities for evaluating reasoning and visual understanding.

% DME dataset allowed the evaluation of consistency at a more local level
% Global vs. local
The primary advantage of this combination of localized questions and consistency is the expansion of consistency evaluation from exclusively global-to-global to global-to-local and local-to-local. Here \textit{global} refers to questions about the entire image, and \textit{local} refers to localized questions. With this expansion, the consistency evaluation becomes more detailed and localized, contributing to the reliability of the models. Consider, for instance, the questions from the DME-VQA~\cite{tascon2022consistency} that refer to the presence of hard exudates in the entire image and in a specific region. Here, once again, we emphasize the dependence of consistency on the answers since answering ``yes" to the global question admits both ``yes" and ``no" responses to the local question, depending on the location of the region. However, answering ``no" to the global question imposes, from the standpoint of consistency, a constraint in the answer to the local questions. In this scenario, and assuming the prediction for the global question is correct, the model should answer ``no" to all region-based questions that inquire about the same biomarker. We contend that this type of compositional evaluation can make the consistency assessment of a model more robust, as its understanding of the image is tested at both global and local levels, revealing possible shortcuts or perception errors.

% local vs. local
In the local-to-local context, certain relevant reasoning failures could be detected (or ensured to be absent). Revisiting the questions about hard exudates in the DME-VQA dataset, we underscore the importance of answers to identical questions concerning cases where a region $r_1$ contains another one, $r_2$, with $area(r_1)>area(r_2)$. Given the question $q = \,$``are there hard exudates in this region?" we observe that an affirmative answer to $q_{r_2}$ implies an affirmative answer to $q_{r_1}$ and that a negative answer to $q_{r_1}$ implies a negative answer to $q_{r_2}$. These implications enable the identification of inconsistencies in which the model alters its prediction when exposed to more or less context, respectively. This, however, should be analyzed carefully since the mentioned implications do not apply to all types of images/questions.

% Highlight the importance of models that can properly answer localized questions.
In both extensions of consistency mentioned, namely global-to-local and local-to-local, the approaches presented in this work emphasize the need for models capable of answering localized questions by considering context while also respecting the precise boundaries and contents of regions. Simultaneously, these models should assimilate knowledge about implication violations to enhance performance and overall reasoning capabilities.

\section{Conclusion}

In conclusion, this thesis has delved into the principles of~\gls{vqa} in the medical domain, focusing on two key aspects: enabling localized queries and enhancing consistency. The exploration of these dimensions of \gls{medvqa} contributes to a nuanced understanding of medical image interpretation and reasoning. The devised methodologies, from introducing localized attention mechanisms and a visual prompting technique for \glspl{mllm} to proposing a logic-centric method for consistency, mark noteworthy advances in addressing the challenges posed by VQA in medical scenarios. Furthermore, several \gls{medvqa} datasets were created and made publicly available, fostering research efforts within the research community in the domains of localized questions and consistency.

The work on localized questions introduces novel ways for users to query specific regions of medical images, supporting a more targeted and informative interaction with the model. This has implications for clinical applications, offering potential benefits in diagnosis, second opinions, and overall medical image analysis.

Conversely, the in-depth considerations regarding consistency in VQA models reveal its critical role in improving reasoning capabilities. The presented methods penalize inconsistent cases at training time, resulting in higher consistency and overall performance during inference, thereby showing the adequacy of the enhancement techniques.

By intertwining the realms of localized queries and consistency, this work paves the way for a comprehensive approach to robustness in \gls{medvqa}. This combination not only refines the assessment but also exposes the models to diverse challenges that go beyond global information, ensuring a more robust and trustworthy performance.

In essence, this thesis contributes to the evolving landscape of medical VQA by introducing approaches that enhance interpretability, reasoning, and reliability. As the field continues to advance with the adoption of larger models, the insights and methodologies presented herein offer valuable perspectives, emphasizing the ongoing quest for more accurate, consistent, and context-aware \gls{medvqa} systems.
 

% Also MLLMs were considered 

% Extensive experimentation on multiple datasets


\chapter{Future Work}
\label{chapter:future_work}

The evolution of \gls{vqa} and \gls{medvqa} has been remarkably rapid in the last years, both at the architectural and data levels. Initially, the structure comprised a simple combination of an \gls{rnn}, a \gls{cnn}, a multiplication operation, and a classifier. Over time, this has transformed into sophisticated \glspl{mllm} with billions of parameters, forming complex stacks of layers with attention mechanisms at their core.

On the data front, datasets have expanded to encompass millions of questions, addressing existing biases and incorporating additional information such as scene graphs~\cite{hudson2019gqa}. The \glspl{llm} responsible for reasoning in \glspl{mllm} are trained on internet-scale datasets with hundreds of billions of words. Despite these advancements, visual understanding and multimodal reasoning remain pertinent topics~\cite{tong2024eyes,wang2024exploring,wei2023enhancing,fan2024muffin} that deserve the attention of the research community.

Taking into account these advancements and the works presented in this thesis, this chapter delineates directions and considerations for future work in the domains of consistency and localized questions.

\newpage

\section{Localized Questions}

% Extension to non-binary questions -> Dataset
Regarding the data used to train models that answer localized questions, a natural progression from our current focus on binary questions is the integration of non-binary inquiries. Our study was limited to binary questions due to the absence of publicly available \gls{vqa} datasets featuring questions about regions or other datasets with spatial annotations beyond segmentation masks. In future developments, the formulation of questions could include a spectrum of topics, including anatomical descriptions (\eg, ``which organ is in this region?"), comparisons (\eg, ``how does this region compare to a healthy counterpart in the image?"), spatial relationships (\eg, ``what structures or organs are adjacent to this region?"), functional questions (\eg, ``what is the organ in the region responsible for?"), pathological descriptions (\eg, ``what pathological findings or abnormalities are in this region?"), diagnostic questions (\eg, ``what diagnostic information can be derived from the organ in this region?"), \etc 

Creating datasets for such diverse questions would require active involvement from clinical experts in the generation of each QA pair, as well as carefully considering inter-expert variability to minimize biases in the datasets. Alternatively, leveraging heavily annotated medical datasets could automate the generation of questions. Ideally, a wide range of modalities such as CT, MRI, X-Ray, and OCT, among others, should be included to enhance the applicability of \gls{medvqa} models in clinical practice. Moreover, questions and answers should accurately reflect the style and specialized terminology employed by medical professionals, a goal that could be feasibly attained through the utilization of \glspl{llm}.


% Evaluation of other image modalities such as X-Ray where maybe data is more abundant
% Minimization of biases
Prioritizing the minimization of biases in the data is imperative, particularly considering the potential impact of object size on answer distribution. For instance, in an image with few and small lesions, generating localized questions about healthy tissue might be easier than those about abnormal tissue. These data balance considerations are crucial to reduce model reliance on spurious correlations based on object location or size.

% Improvements on localized attention
Regarding our method from Chapter~\ref{chapter:locvqa} for localized questions, further developments could involve exploring alternative architectures in the text and image embedding blocks. For instance, for the text encoder, investigating the efficacy of the RWKV architecture~\cite{peng2023rwkv} combining \glspl{rnn} with transformers could be a possibility. Additionally, investigating alternative multimodal fusion approaches, like bilinear pooling, could potentially improve performance.

A more drastic modification could involve adopting an attention pyramid, where attention maps are created at different depths of the visual encoder's features. This might address the potential loss of region detail when applying the resized binary mask to visually attended features when the region shape is complex.

Furthermore, delving deeper into the role of glimpses for localized questions could refine the model's handling of redundancy. Observations during experimental development showed instances where one glimpse focuses on the object mentioned in the question while another highlights a different part of the image. Investigating this aspect can contribute to a more nuanced and effective localized questioning approach.

% Improvements on targeted visual prompting
Concerning our efforts in Chapter~\ref{chapter:locvqallm}, alternative approaches might explore the use of \glspl{cnn} for encoding the image, either independently or in conjunction with \glspl{vit}. Such an approach could facilitate a direct mapping of visual tokens to the input image and potentially allow for a combination of both proposed methods. In this scenario, visual features could undergo filtering based on the target regions (localized attention), enabling the \gls{llm} to receive locally attended visual tokens.

\section{Consistency Enhancement}

% Extension to non-binary questions -> Dataset
% Li-MOD needs a lot of improvement -> Exploit more sophisticated LLMs
Similar to the scenario of localized questions, the incorporation of non-binary questions for consistency enhancement represents a crucial advancement. This extension allows for a more comprehensive examination of the model's reasoning capabilities and language understanding. In our exploration using the Introspect-VQA dataset (Chapter~\ref{chapter:cons_logic}), we exclusively employed binary questions due to the intricacies associated with assigning modal relations to non-binary QA pairs. However, the annotations regarding implications could be acquired during dataset creation or generated by a sophisticated \gls{llm}. Leveraging the recent progress in \glspl{llm}, these models could serve as auxiliary tools for logical reasoning/knowledge. By learning from extensive text data and building associations, \glspl{llm} more closely resemble human learning of implications, which takes place in different forms (experience, deductive and inductive reasoning, error correction, \etc). This, linked to the work of this thesis, corresponds to upgrading LI-MOD in the method from Chapter~\ref{chapter:cons_logic}. An added advantage of using a pre-trained \gls{llm} is its potential for zero-shot operation, drawing on the knowledge abstracted from extensive training data.

% Dynamic tuning of \gamma?
For the work presented in Chapter~\ref{chapter:cons_mainsub}, envisioning a dynamic tuning of the hyperparameter $\gamma$ could be explored. However, this should be performed under the revised definition of consistency (Chapter~\ref{chapter:cons_logic}), considering the limitations associated with the main-sub categorization, as discussed earlier. Finding new functions for the loss term could also represent an interesting avenue for further development.

% Refinement of consistency definitions: handle more nuanced scenarios, consider question types, context, image characteristics.
Refining the consistency definitions offers another potential direction, with a focus on considering the context provided in the image to weigh the applied implications. That is, a  more robust definition could relax the implication relation based on the contents of the image. For instance, in the previously examined example (``is it summer?", ``no") $\leftarrow$ (``is there snow?", "yes"), the implication relation holds for most cases, but exceptions exist. A more nuanced definition of consistency for \gls{vqa} could adapt the implication relation based on the specifics of the image contents: if the image shows snow at the top of a mountain, the chances that the relation does not hold increase as compared to an image showing snow on a street in New York. 

% Logic paper: test other architectures such as OFA, BEiT-2 or some MLLM
With respect to the method in Chapter~\ref{chapter:cons_logic}, testing more advanced architectures, such as OFA~\cite{wang2022ofa}, BEIT-3~\cite{wang2022image}, or other \glspl{vlm}, could be pursued to evaluate the method's effectiveness. In this context, the increased capacity and pre-training of these models are anticipated to lead to enlarged consistency. Our method is expected to provide additional benefits in addressing contradictions beyond what increased overall performance can bring. 

% Human-in-the-loop approaches
Finally, exploring human-in-the-loop approaches represents another avenue where models could learn to avoid contradictions by leveraging error feedback provided by humans.


\section{Bridging Locality and Consistency}

We now explore potential avenues for future research at the intersection of localized questions and consistency enhancement. 

% Studies in real-world clinical environments
In the realm of real-world clinical applications, a promising area for investigation involves evaluating the trustworthiness of \gls{vqa} models. This could entail engaging medical experts with \gls{vqa} models featuring varying levels of consistency to assess their trust in the system. Questions about regions would be included, allowing the experts to probe the model in a more dynamic and interactive way. Such interaction would provide insights into the types of contradictions that may impact specialists' reluctance to adopt these models in clinical practice.

% Medical assistants that combine localized questions and consistency quantification
Another avenue pertinent to clinical environments is the development of \gls{mllm}-based medical assistants with consistency enhancement mechanisms. Due to the availability of large datasets, this could be conceived first for pathology and radiology images, building on recent efforts~\cite{pellegrini2023radialog,lu2023foundational,sun2023pathasst}, but should then be expanded to other modalities and should integrate the option to ask questions about user-defined regions. These assistants would enable users to pose questions of any kind (text-only, image, or region) while ensuring coherence in responses. 

% Datasets
With reference to dataset creation, there is a need for the development of more robust datasets incorporating both localized questions and relation annotations. This development would address the challenges outlined previously for each scenario, ensuring the availability of diverse QA pairs that put the model to the test in terms of relations between propositions that also involve prior knowledge (\eg, presence of biomarkers that imply a certain disease). 

% What about expanding the implications to more complex reasoning techniques? E.g. silogism
One significant advancement over implication relations involves the inclusion of more than two propositions. Scenarios could be envisioned where multiple propositions collectively lead to a conclusion (\ie, $A_1 \wedge A_2 \wedge ... \wedge A_N \rightarrow B$). Here, the propositions $A_i, \, i= 1,...,N $ can be treated as one single proposition, allowing the application of the same definition of consistency from Chapter~\ref{chapter:cons_logic}. If a model assigns \textit{true} to $A_i, i= 1,...,N$ but \textit{false} to $B$, the set of propositions will be considered inconsistent. Importantly, the individual pairs $\{ (A_1, B), ..., (A_N, B) \}$ are not considered inconsistent, since the implication relation requires the evaluation of all propositions simultaneously. This can be useful for diagnosis decisions requiring the presence of $N$ biomarkers in the image, or in certain regions. An extension could also be made for cases where the question contains prior information about the patient not present in the image, such as findings from previous images (\ie, longitudinal information) or blood work results. In such scenarios, the question would assert a proposition, and could be included in the consistency evaluation as a complement to the propositions implied by QA pairs.  

% Evaluation metrics, e.g. give different weights to global-to-global and local-to-local
Enhancing the evaluation of consistency in the context of localized questions may require optimizing metrics. Assigning different weights to inconsistencies based on the extent of the visual information associated with the violated implication relation could be a strategic approach. Deeming local-to-local inconsistencies as more critical than global-to-global inconsistencies seems intuitive, given that the former involve less visual information and should be more manageable for the model to address. Global questions often require higher-level reasoning and the composition of various perception tasks, hence, imposing more significant penalties on seemingly straightforward cases appears justifiable. This, of course, rests on the assumption that the model can answer both types of questions at a comparable level.

% Explainability and interpretability methods that summarize the performance. e.g. identify image regions that produce contradictions.
Another possible direction for future work in localized questions and consistency is the development of explainability and interpretability methods. These methods aim to summarize the model's performance and attempt to explain its predictions. For instance, global predictions could be deconstructed into local predictions about non-overlapping regions, showcasing the extent to which the model's interpretation of the entire image can be decomposed into an understanding of its constituent parts (compositional \gls{vqa}). Other approaches could be devised to test a model's comprehension of an image by identifying regions for which the model's answers contradict the response to a global question. Such an approach may uncover objects or structures that the model confuses with those mentioned in the question, contributing to a deeper understanding of model behavior.
