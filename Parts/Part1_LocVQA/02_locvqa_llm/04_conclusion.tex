\section{Conclusion}
\label{sec:locvqallm_conclusion}

In this work, we introduced a novel approach to enable localized questions in multimodal \glspl{llm} for the tasks of \gls{vqa}. Our proposed approach involves the utilization of targeted visual prompting, granting the model access not only to the region and its context within the image but also to an isolated version of the region. By doing so, we allow two perspectives to be encoded in the prompt and allow more fine-grained information to be leveraged. Our approach demonstrates enhanced performance across all evaluated datasets compared to a variety of baselines. Analysis of the results highlights how biases in the datasets can be interpreted and qualitative examples shown depict failure modes of our method. Future works include extending the methodology to accommodate multiple images and enabling the use of comparison questions.